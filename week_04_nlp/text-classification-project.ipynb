{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "_Project for week 4, by Jan KÃ¼hn, April 2023_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project task and outline\n",
    "\n",
    "In this project, we will build a text classification model on song lyrics. The task is to predict the artist from a piece of text. To train such a model, you first need to collect your own lyrics dataset:\n",
    "\n",
    "- Download a HTML page with links to songs\n",
    "- Extract hyperlinks of song pages\n",
    "- Download and extract the song lyrics\n",
    "- Vectorize the text using the Bag Of Words method\n",
    "- Train a classification model that predicts the artist from a piece of text\n",
    "- Refactor the code into functions\n",
    "- Write a simple command-line interface for the program\n",
    "- Upload your code to GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import STOPWORDS, WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPE_PATH = \"scrape/\"\n",
    "SCRAPE_SONG_LIST = False\n",
    "SCRAPE_SONGS = False\n",
    "PARSE_HTML = False\n",
    "CREATE_WORDCLOUDS = False\n",
    "SLEEP_SEC = 3\n",
    "HEADER = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0\"\n",
    "}\n",
    "artist_urls = {\n",
    "    \"Eels\": \"https://www.lyrics.com/artist.php?name=Eels&aid=182509&o=1\",\n",
    "    \"Rage Against the Machine\": \"https://www.lyrics.com/artist.php?name=Rage-Against-the-Machine&aid=23206&o=1\",\n",
    "    \"Adele\": \"https://www.lyrics.com/artist.php?name=Adele&aid=861756&o=1\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape & Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_artist(artist: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to shorten the artist name.\n",
    "    \"\"\"\n",
    "    return \"\".join(re.findall(r'\\b\\w', artist)).lower() if len(artist.split(\" \")) > 1 else artist.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist_song_list(artist_urls: dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Function to scrape song list from a website and save them as files.\n",
    "    \"\"\"\n",
    "\n",
    "    for artist, url in artist_urls.items():\n",
    "        # Create directory for scraped files if it doesn't exist\n",
    "        if not os.path.exists(SCRAPE_PATH):\n",
    "            os.makedirs(SCRAPE_PATH)\n",
    "\n",
    "        file_name = f\"{shorten_artist(artist)}_full_song_list.html\"\n",
    "\n",
    "        # Do nothing if file exists already\n",
    "        if os.path.isfile(os.path.join(SCRAPE_PATH, file_name)):\n",
    "            print(f\"Skipped existing file {file_name}.\")\n",
    "            continue\n",
    "\n",
    "        # If file does not exist, fetch it\n",
    "        response = requests.get(url, HEADER, allow_redirects=False)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(SCRAPE_PATH + file_name, \"w\") as f:\n",
    "                f.write(response.text)\n",
    "\n",
    "            print(f\"Song list for {artist} written to file {SCRAPE_PATH}{file_name}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: Response code {response.status_code} for URL {url}.\")\n",
    "\n",
    "        time.sleep(SLEEP_SEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_urls(song_urls: dict[str, list]) -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Function to remove duplicate URLs.\n",
    "    \"\"\"\n",
    "    song_urls_clean = {}\n",
    "\n",
    "    for artist, urls in song_urls.items():\n",
    "        urls_clean = []\n",
    "        count_append = 0\n",
    "        count_remove = 0\n",
    "\n",
    "        for url in urls:\n",
    "            end_of_url = \"/\".join(url.rsplit(\"/\", 2)[1:3])\n",
    "\n",
    "            # Check if string already exists in list\n",
    "            if not any(end_of_url in c for c in urls_clean):\n",
    "                urls_clean.append(url)\n",
    "                count_append += 1\n",
    "            else:\n",
    "                count_remove += 1\n",
    "\n",
    "        print(\n",
    "            f\"{count_remove} duplicates removed, leaving {count_append} URLs for {artist}.\"\n",
    "        )\n",
    "\n",
    "        song_urls_clean[artist] = urls_clean\n",
    "\n",
    "    return song_urls_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_urls(artist_urls: dict[str, str]) -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Function to get song URLs from HTML files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scrape HTML files containing URLs to song lyrics\n",
    "    if SCRAPE_SONG_LIST:\n",
    "        scrape_artist_song_list(artist_urls)\n",
    "\n",
    "    song_urls = {}\n",
    "\n",
    "    for artist in artist_urls:\n",
    "        file_name = f\"{shorten_artist(artist)}_full_song_list.html\"\n",
    "        count = 0\n",
    "\n",
    "        with open(SCRAPE_PATH + file_name, \"r\") as f:\n",
    "            html = f.read()\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            parsed_urls = []\n",
    "\n",
    "            for row in soup.find(\"table\", class_=\"tdata\").find_all(\"tr\"):\n",
    "                try:\n",
    "                    # Get URL from href\n",
    "                    url = row.find(\"td\").find(\"a\", href=True)[\"href\"]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Append to list\n",
    "                parsed_urls.append(\"https://www.lyrics.com\" + url)\n",
    "                count += 1\n",
    "\n",
    "        song_urls[artist] = parsed_urls\n",
    "        print(f\"Added {count} URLs for artist {artist}.\")\n",
    "\n",
    "    # Remove duplicate URLS\n",
    "    song_urls_clean = remove_duplicate_urls(song_urls)\n",
    "\n",
    "    return song_urls_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_songs_to_files(song_urls: dict[str, list]) -> None:\n",
    "    \"\"\"\n",
    "    Function to scrape songs and save them locally.\n",
    "    \"\"\"\n",
    "    for artist, urls in song_urls.items():\n",
    "        path = SCRAPE_PATH + shorten_artist(artist) + \"/\"\n",
    "        count_skipped = 0\n",
    "\n",
    "        # Create directory for scraped files if it doesn't exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        for url in urls:\n",
    "            file_name = f\"{shorten_artist(artist)}-{url.split('/')[-1]}.html\"\n",
    "\n",
    "            # Do nothing if file exists already\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                count_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # GET file\n",
    "            response = requests.get(url, HEADER, allow_redirects=False)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                with open(path + file_name, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "\n",
    "                print(f\"File {path + file_name} for {artist} written to file.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Error: Response code {response.status_code} for URL {url}.\")\n",
    "\n",
    "            time.sleep(SLEEP_SEC)\n",
    "\n",
    "        print(f\"Skipped {count_skipped} existing files for artist {artist}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html: str, source: str = \"\") -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Function to parse HTML and extract title, artist and lyrics\n",
    "    \"\"\"\n",
    "\n",
    "    title, artist, lyrics = \"\", \"\", \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract title, artits, and lyrics\n",
    "    try:\n",
    "        title = soup.h1.text.strip()\n",
    "    except Exception:\n",
    "        print(f\"Error parsing title at {source}.\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        artist = soup.find(\"h3\", class_=\"lyric-artist\").text.strip()\n",
    "    except Exception:\n",
    "        print(f\"Error parsing artist at {source}.\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        lyrics = soup.find(id=\"lyric-body-text\").text.strip()\n",
    "    except Exception:\n",
    "        print(f\"Error parsing lyrics at {source}.\")\n",
    "        pass\n",
    "\n",
    "    return title, artist, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_from_file(path_html: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Function to scrape one single song lyric from html file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_html, \"r\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    title, artist, lyrics = parse_html(html, path_html)\n",
    "\n",
    "    return title, artist, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_from_url(url: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Function to scrape one single song lyric from URL\n",
    "    \"\"\"\n",
    "\n",
    "    title, artist, lyrics = \"\", \"\", \"\"\n",
    "\n",
    "    response = requests.get(url, HEADER, allow_redirects=False)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        title, artist, lyrics = parse_html(response.text, url)\n",
    "    else:\n",
    "        print(f\"Error: Response code {response.status_code} for URL {url}.\")\n",
    "\n",
    "    return title, artist, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_parse(artists: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Function to get file names in the scrape directory.\n",
    "    \"\"\"\n",
    "\n",
    "    all_files = {}\n",
    "\n",
    "    for artist in artists:\n",
    "        # Directory holding HTML files\n",
    "        path_html_files = SCRAPE_PATH + \"/\" + shorten_artist(artist)\n",
    "\n",
    "        all_files[artist] = [\n",
    "            f\n",
    "            for f in os.listdir(path_html_files)\n",
    "            if os.path.isfile(os.path.join(path_html_files, f)) and f.endswith(\".html\")\n",
    "        ]\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lyrics_from_files(song_urls: dict[str, list[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to parse lyrics from existing files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scrape lyrics from URLS\n",
    "    if SCRAPE_SONGS:\n",
    "        scrape_songs_to_files(song_urls)\n",
    "\n",
    "    # Create empty DataFrame\n",
    "    songs = pd.DataFrame(columns=[\"title\", \"artist\", \"lyrics\"])\n",
    "\n",
    "    # Get file names\n",
    "    files_to_parse = get_files_to_parse(list(song_urls.keys()))\n",
    "    \n",
    "    # Loop through file names and parse HTML\n",
    "    for artist, files in files_to_parse.items():\n",
    "        for f in files:\n",
    "            path_html_file = SCRAPE_PATH + shorten_artist(artist) + \"/\" + f\n",
    "            title_, artist_, lyrics_ = get_lyrics_from_file(path_html_file)\n",
    "            songs.loc[len(songs)] = [title_, artist_, lyrics_]\n",
    "            \n",
    "    # Clean data\n",
    "    songs_clean = clean_data(songs)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    file_name_csv_clean = \"data/songs_clean.csv\"\n",
    "    songs_clean.to_csv(file_name_csv_clean)\n",
    "\n",
    "    print(f\"Saved {len(songs_clean)} songs to {file_name_csv_clean}\")\n",
    "\n",
    "    return songs_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to clean the data.\n",
    "    \"\"\"\n",
    "    # Remove all rows where lyrics cell is empty\n",
    "    df_ = df_[df_[\"lyrics\"].notna()]\n",
    "    df_ = df_[df_[\"lyrics\"] != \"\"]\n",
    "\n",
    "    # Remove all songs that are not exactly by artists specified\n",
    "    filters = []\n",
    "    for a in artist_urls:\n",
    "        filters.append(df_[\"artist\"].str.lower() == a.lower())\n",
    "    filter = [any(sublist) for sublist in zip(*filters)]\n",
    "    df_ = df_[filter]\n",
    "\n",
    "    # Remove all rows where title ends with ]\n",
    "    df_ = df_[df_[\"title\"].str[-1] != \"]\"]\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lyrics_to_lines(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to split the lyrics by line.\n",
    "    \"\"\"\n",
    "    df_ = (\n",
    "        # Set columns not to be touched as index\n",
    "        df_.set_index([\"title\", \"artist\"])\n",
    "        # Split and explode the lyrics by newline\n",
    "        .apply(lambda x: x.str.split(\"\\n\").explode())\n",
    "        # Reset index\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Remove rows without lyrics in them\n",
    "    df_ = df_[df_[\"lyrics\"].notna()]\n",
    "    df_ = df_[df_[\"lyrics\"] != \"\"]\n",
    "\n",
    "    df_.to_csv(\"data/songs_by_line.csv\")\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_create_img(text: str, width: int = 2000, height: int = 1500):\n",
    "    \n",
    "    # Define font to be used (downloaded from https://www.cufonfonts.com/font/boldova)\n",
    "    font_file = \"Boldova.ttf\"\n",
    "    \n",
    "    if not os.path.isfile(font_file):\n",
    "        print(\"Error: Font file not found.\")\n",
    "        return None\n",
    "    \n",
    "    font = ImageFont.truetype(\"Boldova.ttf\", size=600)\n",
    "\n",
    "    # Create image\n",
    "    img = Image.new(\"RGB\", (width, height), color=\"white\")\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Calculate coordinates to center the text\n",
    "    text_width, text_height = img_draw.textsize(text, font=font)\n",
    "    x_text = int((width - text_width) / 2)\n",
    "    y_text = int((height - text_height) / 2)\n",
    "\n",
    "    # Add text\n",
    "    img_draw.text((x_text, y_text), text, font=font, fill=(0, 0, 0))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(corpus: str, name: str, shape: str = \"rect\") -> None:\n",
    "    # Some settings\n",
    "    width = 2000\n",
    "    height = 1000\n",
    "\n",
    "    # Create shapes\n",
    "    if shape == \"circle\":\n",
    "        # From https://www.python-lernen.de/wordcloud-erstellen-python.htm\n",
    "        x, y = np.ogrid[:1000, :1000]\n",
    "        mask = (x - 500) ** 2 + (y - 500) ** 2 > 400**2\n",
    "        mask = 255 * mask.astype(int)\n",
    "\n",
    "        # Change width to get a square\n",
    "        width = height\n",
    "    elif shape == \"text\":\n",
    "        # Change width to get a wide rectangle\n",
    "        height = int(width / 2)\n",
    "\n",
    "        # Create image with text\n",
    "        wordcloud_img = wordcloud_create_img(name, width=width, height=height)\n",
    "        if wordcloud_img is None:\n",
    "            return None\n",
    "        mask = np.array(wordcloud_img)\n",
    "    else:\n",
    "        mask = None\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        random_state=1,\n",
    "        background_color=\"white\",\n",
    "        # colormap=\"Pastel1\",\n",
    "        collocations=False,\n",
    "        stopwords=STOPWORDS,\n",
    "        mask=mask,\n",
    "        contour_color=\"#ccc\",\n",
    "        contour_width=2,\n",
    "    ).generate(corpus)\n",
    "\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud)\n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    # Save as file\n",
    "    plt.savefig(f\"wordclouds/wordcloud-{name}-{shape}.png\", dpi=72, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARSE_HTML:\n",
    "    # Get song URLs\n",
    "    song_urls = get_song_urls(artist_urls)\n",
    "\n",
    "    # Parse lyrics from file and save them in a CSV file\n",
    "    songs = parse_lyrics_from_files(song_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"data/songs_clean.csv\", index_col=0)\n",
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = convert_lyrics_to_lines(songs)\n",
    "df_corpus[\"artist\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_WORDCLOUDS:\n",
    "    corpus = \" \".join(df_corpus[df_corpus[\"artist\"] == \"Eels\"][\"lyrics\"])\n",
    "    plot_wordcloud(corpus, name=\"Eels\", shape=\"circle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rage Against the Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_WORDCLOUDS:\n",
    "    corpus = \" \".join(\n",
    "        df_corpus[df_corpus[\"artist\"] == \"Rage Against the Machine\"][\"lyrics\"]\n",
    "    )\n",
    "    plot_wordcloud(corpus, name=\"ratm\", shape=\"text\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adele\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_WORDCLOUDS:\n",
    "    corpus = \" \".join(\n",
    "        df_corpus[df_corpus[\"artist\"] == \"Adele\"][\"lyrics\"]\n",
    "    )\n",
    "    plot_wordcloud(corpus, name=\"Adele\", shape=\"circle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data if not already downloaded\n",
    "#nltk.download(\"wordnet\", download_dir=\"/home/jan/.miniconda3/envs/jupyter/nltk_data\")\n",
    "#nltk.download('stopwords', download_dir=\"/home/jan/.miniconda3/envs/jupyter/nltk_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(df_c: pd.DataFrame) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Function to prepare the corpus from a dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = []\n",
    "    labels = []\n",
    "\n",
    "    # Create list of song lines and labels\n",
    "    for artist in df_c[\"artist\"].unique():\n",
    "        song_lines = df_c[df_c[\"artist\"] == artist][\"lyrics\"]\n",
    "        \n",
    "        for line in song_lines:\n",
    "            corpus.append(line)\n",
    "\n",
    "        for i in range(len(song_lines)):\n",
    "            labels.append(artist)\n",
    "\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus_: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function to preprocess the data for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    corpus_ = [s.lower().strip() for s in corpus_]\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    corpus_clean = []\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for doc in corpus_:\n",
    "        tokens = tokenizer.tokenize(text=doc)\n",
    "        clean_doc = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        corpus_clean.append(clean_doc)\n",
    "    \n",
    "    return corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(lyrics: list[str], predictions: list[str], probabilities: list[float]) -> None:\n",
    "    \"\"\"\n",
    "    Function to print the results of a prediction.\n",
    "    \"\"\"\n",
    "    prob_phrases = []\n",
    "    for p in probabilities:\n",
    "        if p < 0.6:\n",
    "            prob_phrases.append(\"I guess\")\n",
    "        elif p < 0.75:\n",
    "            prob_phrases.append(\"I believe\")\n",
    "        elif p < 0.9:\n",
    "            prob_phrases.append(\"I am pretty sure\")\n",
    "        else:\n",
    "            prob_phrases.append(\"I am positive\")\n",
    "\n",
    "    for l, pred, prob, phrase in zip(lyrics, predictions, probabilities, prob_phrases):\n",
    "        print(f\"Line: {l}\\n{phrase} that line is from a {pred} song ({prob:.0%} sure)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus and labels\n",
    "CORPUS, LABELS = prepare_corpus(df_corpus)\n",
    "assert(len(CORPUS) == len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "CORPUS_CLEAN = preprocess_corpus(CORPUS)\n",
    "assert(len(CORPUS_CLEAN) == len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words\n",
    "STOPWORDS = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"tdidf\", TfidfVectorizer(stop_words=STOPWORDS)),\n",
    "        (\"nb\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"nb__alpha\": [0.1, 0.5, 1, 2, 3],\n",
    "    \"nb__fit_prior\": [True, False],\n",
    "    \"tdidf__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# initial time\n",
    "ti = time.time()\n",
    "\n",
    "# grid-search cross-validation\n",
    "gscv.fit(CORPUS_CLEAN, LABELS)\n",
    "\n",
    "# final time\n",
    "tf = time.time()\n",
    "\n",
    "# time taken\n",
    "print(f\"time taken: {round(tf-ti,2)} sec\")\n",
    "\n",
    "print(f\"Best parameters: {gscv.best_params_}\")\n",
    "print(f\"Best score: {round(gscv.best_score_,6)}\")\n",
    "\n",
    "model = gscv.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with the vectorized data\n",
    "model.fit(CORPUS_CLEAN, LABELS)\n",
    "\n",
    "# Check score\n",
    "model.score(CORPUS_CLEAN, LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained model to predict for new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = [\n",
    "    \"From the era of terror, check this photo lens\",\n",
    "    \"beautiful freak\",\n",
    "    \"Fuck you I won't do what you tell me\",\n",
    "    \"Bombtrack\",\n",
    "    \"the mistakes of my youth\",\n",
    "    \"Check it, since fifteen hundred and sixteen, minds attacked and overseen\",\n",
    "    \"Shock around tha clock, from noon 'til noon\",\n",
    "    \"When I came into this world they slapped me\",\n",
    "    \"Or should I just keep chasing pavements?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "lyrics_clean = preprocess_corpus(lyrics)\n",
    "\n",
    "# Get results\n",
    "predictions = model.predict(lyrics_clean)\n",
    "probabilities = [p.max() for p in model.predict_proba(lyrics_clean)]\n",
    "\n",
    "# Print results\n",
    "print_results(lyrics, predictions, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_asking = True\n",
    "\n",
    "while keep_asking:\n",
    "    user_input = input(\"Enter a line from a song by the Eels, Adele, or Rage Against the Machine\")\n",
    "\n",
    "    if user_input in [\"quit\", \"q\", \"exit\"]:\n",
    "        keep_asking = False\n",
    "        continue\n",
    "\n",
    "    lyrics = [user_input]\n",
    "\n",
    "    # Preprocess\n",
    "    lyrics_clean = preprocess_corpus(lyrics)\n",
    "\n",
    "    # Get results\n",
    "    predictions = model.predict(lyrics_clean)\n",
    "    probabilities = [p.max() for p in model.predict_proba(lyrics_clean)]\n",
    "\n",
    "    # Print results\n",
    "    print_results(lyrics, predictions, probabilities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
